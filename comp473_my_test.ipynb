{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MyQ4QFbZVhuG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get training and validation(=testing) data\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    '/content/drive/MyDrive/comp473ProjectData/TrainSet1', \n",
        "    image_size = (48,48), # our original dataset is 48 pixels by 48 pixels\n",
        "    batch_size = 12 # pick 12 images and trin until all dataset is used <= repeat this for each epoche\n",
        ")\n",
        "\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    '/content/drive/MyDrive/comp473ProjectData/TestSet1', \n",
        "    image_size = (48,48), \n",
        "    batch_size = 12\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB6z6XFaWdcv",
        "outputId": "c1b8540b-5e75-4fdd-c50e-6f4d89436a0e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 693 files belonging to 7 classes.\n",
            "Found 288 files belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds # form shape=(None, 48, 48, 3) <- 3 represent that the photo is treated as colored. one pixel will have [R G B] values between 0~255"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDTPOOojvWc4",
        "outputId": "307a2019-d9ec-4c0c-9bfa-7c02d28c28b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(None, 48, 48, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try to output one of our image\n",
        "\n",
        "for i, answer in train_ds.take(1): #take(1): takes first batch. \n",
        "\n",
        "  print(i) # outputs 12(=batch size) photos as numpy array ,shape=(12, 48, 48, 3) means 12 photos, each photo size(48,48 pixels), 3=color= each cell represented as [R G B]\n",
        "  print(answer) # outputs correct answer for current batch [ 5 2 6 ...] <- first photo in this batch is in class 5, second photo in this patch is in class 2.. \n",
        "\n",
        "plt.imshow(i[0].numpy().astype('uint8')) # output the first photo using matplotlib\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nlxVewnivXWB",
        "outputId": "cdfa1795-48ef-413a-c428-962b243be067"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[ 42.  42.  42.]\n",
            "   [ 44.  44.  44.]\n",
            "   [ 59.  59.  59.]\n",
            "   ...\n",
            "   [ 49.  49.  49.]\n",
            "   [ 49.  49.  49.]\n",
            "   [ 51.  51.  51.]]\n",
            "\n",
            "  [[ 39.  39.  39.]\n",
            "   [ 42.  42.  42.]\n",
            "   [ 45.  45.  45.]\n",
            "   ...\n",
            "   [ 40.  40.  40.]\n",
            "   [ 49.  49.  49.]\n",
            "   [ 47.  47.  47.]]\n",
            "\n",
            "  [[ 35.  35.  35.]\n",
            "   [ 32.  32.  32.]\n",
            "   [ 34.  34.  34.]\n",
            "   ...\n",
            "   [ 38.  38.  38.]\n",
            "   [ 43.  43.  43.]\n",
            "   [ 44.  44.  44.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 80.  80.  80.]\n",
            "   [ 87.  87.  87.]\n",
            "   [ 83.  83.  83.]\n",
            "   ...\n",
            "   [237. 237. 237.]\n",
            "   [189. 189. 189.]\n",
            "   [ 81.  81.  81.]]\n",
            "\n",
            "  [[ 84.  84.  84.]\n",
            "   [ 87.  87.  87.]\n",
            "   [ 81.  81.  81.]\n",
            "   ...\n",
            "   [252. 252. 252.]\n",
            "   [159. 159. 159.]\n",
            "   [ 73.  73.  73.]]\n",
            "\n",
            "  [[ 84.  84.  84.]\n",
            "   [ 84.  84.  84.]\n",
            "   [ 80.  80.  80.]\n",
            "   ...\n",
            "   [248. 248. 248.]\n",
            "   [149. 149. 149.]\n",
            "   [ 68.  68.  68.]]]\n",
            "\n",
            "\n",
            " [[[ 20.  20.  20.]\n",
            "   [  4.   4.   4.]\n",
            "   [ 10.  10.  10.]\n",
            "   ...\n",
            "   [ 81.  81.  81.]\n",
            "   [ 93.  93.  93.]\n",
            "   [ 96.  96.  96.]]\n",
            "\n",
            "  [[  6.   6.   6.]\n",
            "   [  7.   7.   7.]\n",
            "   [  7.   7.   7.]\n",
            "   ...\n",
            "   [ 42.  42.  42.]\n",
            "   [ 92.  92.  92.]\n",
            "   [ 95.  95.  95.]]\n",
            "\n",
            "  [[  4.   4.   4.]\n",
            "   [  8.   8.   8.]\n",
            "   [  7.   7.   7.]\n",
            "   ...\n",
            "   [  8.   8.   8.]\n",
            "   [ 79.  79.  79.]\n",
            "   [ 94.  94.  94.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  2.   2.   2.]\n",
            "   [  7.   7.   7.]\n",
            "   [  8.   8.   8.]\n",
            "   ...\n",
            "   [  6.   6.   6.]\n",
            "   [  4.   4.   4.]\n",
            "   [ 11.  11.  11.]]\n",
            "\n",
            "  [[  0.   0.   0.]\n",
            "   [  5.   5.   5.]\n",
            "   [  7.   7.   7.]\n",
            "   ...\n",
            "   [  4.   4.   4.]\n",
            "   [  3.   3.   3.]\n",
            "   [  4.   4.   4.]]\n",
            "\n",
            "  [[  0.   0.   0.]\n",
            "   [  5.   5.   5.]\n",
            "   [  6.   6.   6.]\n",
            "   ...\n",
            "   [  2.   2.   2.]\n",
            "   [  3.   3.   3.]\n",
            "   [  3.   3.   3.]]]\n",
            "\n",
            "\n",
            " [[[ 98.  98.  98.]\n",
            "   [114. 114. 114.]\n",
            "   [ 69.  69.  69.]\n",
            "   ...\n",
            "   [139. 139. 139.]\n",
            "   [116. 116. 116.]\n",
            "   [148. 148. 148.]]\n",
            "\n",
            "  [[ 95.  95.  95.]\n",
            "   [123. 123. 123.]\n",
            "   [ 92.  92.  92.]\n",
            "   ...\n",
            "   [138. 138. 138.]\n",
            "   [129. 129. 129.]\n",
            "   [168. 168. 168.]]\n",
            "\n",
            "  [[ 91.  91.  91.]\n",
            "   [107. 107. 107.]\n",
            "   [133. 133. 133.]\n",
            "   ...\n",
            "   [133. 133. 133.]\n",
            "   [115. 115. 115.]\n",
            "   [142. 142. 142.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[114. 114. 114.]\n",
            "   [116. 116. 116.]\n",
            "   [120. 120. 120.]\n",
            "   ...\n",
            "   [134. 134. 134.]\n",
            "   [134. 134. 134.]\n",
            "   [133. 133. 133.]]\n",
            "\n",
            "  [[114. 114. 114.]\n",
            "   [116. 116. 116.]\n",
            "   [119. 119. 119.]\n",
            "   ...\n",
            "   [135. 135. 135.]\n",
            "   [134. 134. 134.]\n",
            "   [130. 130. 130.]]\n",
            "\n",
            "  [[113. 113. 113.]\n",
            "   [116. 116. 116.]\n",
            "   [117. 117. 117.]\n",
            "   ...\n",
            "   [134. 134. 134.]\n",
            "   [132. 132. 132.]\n",
            "   [129. 129. 129.]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[121. 121. 121.]\n",
            "   [105. 105. 105.]\n",
            "   [ 88.  88.  88.]\n",
            "   ...\n",
            "   [147. 147. 147.]\n",
            "   [143. 143. 143.]\n",
            "   [146. 146. 146.]]\n",
            "\n",
            "  [[106. 106. 106.]\n",
            "   [ 95.  95.  95.]\n",
            "   [ 89.  89.  89.]\n",
            "   ...\n",
            "   [155. 155. 155.]\n",
            "   [157. 157. 157.]\n",
            "   [151. 151. 151.]]\n",
            "\n",
            "  [[ 95.  95.  95.]\n",
            "   [ 75.  75.  75.]\n",
            "   [ 83.  83.  83.]\n",
            "   ...\n",
            "   [167. 167. 167.]\n",
            "   [163. 163. 163.]\n",
            "   [151. 151. 151.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 99.  99.  99.]\n",
            "   [101. 101. 101.]\n",
            "   [101. 101. 101.]\n",
            "   ...\n",
            "   [117. 117. 117.]\n",
            "   [118. 118. 118.]\n",
            "   [126. 126. 126.]]\n",
            "\n",
            "  [[ 99.  99.  99.]\n",
            "   [101. 101. 101.]\n",
            "   [101. 101. 101.]\n",
            "   ...\n",
            "   [117. 117. 117.]\n",
            "   [118. 118. 118.]\n",
            "   [124. 124. 124.]]\n",
            "\n",
            "  [[100. 100. 100.]\n",
            "   [100. 100. 100.]\n",
            "   [100. 100. 100.]\n",
            "   ...\n",
            "   [117. 117. 117.]\n",
            "   [120. 120. 120.]\n",
            "   [119. 119. 119.]]]\n",
            "\n",
            "\n",
            " [[[ 52.  52.  52.]\n",
            "   [ 60.  60.  60.]\n",
            "   [ 53.  53.  53.]\n",
            "   ...\n",
            "   [ 62.  62.  62.]\n",
            "   [ 67.  67.  67.]\n",
            "   [117. 117. 117.]]\n",
            "\n",
            "  [[ 52.  52.  52.]\n",
            "   [ 58.  58.  58.]\n",
            "   [ 49.  49.  49.]\n",
            "   ...\n",
            "   [ 62.  62.  62.]\n",
            "   [ 63.  63.  63.]\n",
            "   [ 95.  95.  95.]]\n",
            "\n",
            "  [[ 53.  53.  53.]\n",
            "   [ 54.  54.  54.]\n",
            "   [ 45.  45.  45.]\n",
            "   ...\n",
            "   [ 61.  61.  61.]\n",
            "   [ 66.  66.  66.]\n",
            "   [ 78.  78.  78.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[116. 116. 116.]\n",
            "   [118. 118. 118.]\n",
            "   [119. 119. 119.]\n",
            "   ...\n",
            "   [151. 151. 151.]\n",
            "   [152. 152. 152.]\n",
            "   [151. 151. 151.]]\n",
            "\n",
            "  [[117. 117. 117.]\n",
            "   [118. 118. 118.]\n",
            "   [120. 120. 120.]\n",
            "   ...\n",
            "   [149. 149. 149.]\n",
            "   [152. 152. 152.]\n",
            "   [149. 149. 149.]]\n",
            "\n",
            "  [[117. 117. 117.]\n",
            "   [117. 117. 117.]\n",
            "   [113. 113. 113.]\n",
            "   ...\n",
            "   [122. 122. 122.]\n",
            "   [149. 149. 149.]\n",
            "   [149. 149. 149.]]]\n",
            "\n",
            "\n",
            " [[[ 19.  19.  19.]\n",
            "   [ 15.  15.  15.]\n",
            "   [ 31.  31.  31.]\n",
            "   ...\n",
            "   [ 11.  11.  11.]\n",
            "   [ 58.  58.  58.]\n",
            "   [ 69.  69.  69.]]\n",
            "\n",
            "  [[ 11.  11.  11.]\n",
            "   [ 28.  28.  28.]\n",
            "   [ 15.  15.  15.]\n",
            "   ...\n",
            "   [  6.   6.   6.]\n",
            "   [ 46.  46.  46.]\n",
            "   [ 69.  69.  69.]]\n",
            "\n",
            "  [[ 10.  10.  10.]\n",
            "   [ 15.  15.  15.]\n",
            "   [  0.   0.   0.]\n",
            "   ...\n",
            "   [  3.   3.   3.]\n",
            "   [ 33.  33.  33.]\n",
            "   [ 68.  68.  68.]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 48.  48.  48.]\n",
            "   [ 48.  48.  48.]\n",
            "   [ 49.  49.  49.]\n",
            "   ...\n",
            "   [ 61.  61.  61.]\n",
            "   [ 59.  59.  59.]\n",
            "   [ 60.  60.  60.]]\n",
            "\n",
            "  [[ 48.  48.  48.]\n",
            "   [ 50.  50.  50.]\n",
            "   [ 50.  50.  50.]\n",
            "   ...\n",
            "   [ 60.  60.  60.]\n",
            "   [ 62.  62.  62.]\n",
            "   [ 63.  63.  63.]]\n",
            "\n",
            "  [[ 49.  49.  49.]\n",
            "   [ 51.  51.  51.]\n",
            "   [ 52.  52.  52.]\n",
            "   ...\n",
            "   [ 64.  64.  64.]\n",
            "   [ 65.  65.  65.]\n",
            "   [ 64.  64.  64.]]]], shape=(12, 48, 48, 3), dtype=float32)\n",
            "tf.Tensor([0 6 2 6 1 6 2 4 4 1 1 2], shape=(12,), dtype=int32)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess colored data\n",
        "# since the numbers are between 0~255 for each R, G, and B, it is slower to train and calculate weights.\n",
        "# it is better to divide each number with 255, so each value resides within 0~1\n",
        "\n",
        "def processColoredData(i, answer):\n",
        "  i=tf.cast(i/255.0, tf.float32) # devide i by 255, resulting data type should be float\n",
        "  return i, answer\n",
        "\n",
        "train_ds = train_ds.map(processColoredData)\n",
        "val_ds = val_ds.map(processColoredData)"
      ],
      "metadata": {
        "id": "pEoV-cdH1lGq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try to output our image if 'processColoredData' function is working well.\n",
        "for i, answer in train_ds.take(1): #take first batch \n",
        "  print(i) # now you can see that the values are compressed btw 0~1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bid7gMcW1se5",
        "outputId": "b760f03f-92c5-49d5-cf67-54a4d31e2849"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[0.4745098  0.4745098  0.4745098 ]\n",
            "   [0.4117647  0.4117647  0.4117647 ]\n",
            "   [0.34509805 0.34509805 0.34509805]\n",
            "   ...\n",
            "   [0.5764706  0.5764706  0.5764706 ]\n",
            "   [0.56078434 0.56078434 0.56078434]\n",
            "   [0.57254905 0.57254905 0.57254905]]\n",
            "\n",
            "  [[0.41568628 0.41568628 0.41568628]\n",
            "   [0.37254903 0.37254903 0.37254903]\n",
            "   [0.34901962 0.34901962 0.34901962]\n",
            "   ...\n",
            "   [0.60784316 0.60784316 0.60784316]\n",
            "   [0.6156863  0.6156863  0.6156863 ]\n",
            "   [0.5921569  0.5921569  0.5921569 ]]\n",
            "\n",
            "  [[0.37254903 0.37254903 0.37254903]\n",
            "   [0.29411766 0.29411766 0.29411766]\n",
            "   [0.3254902  0.3254902  0.3254902 ]\n",
            "   ...\n",
            "   [0.654902   0.654902   0.654902  ]\n",
            "   [0.6392157  0.6392157  0.6392157 ]\n",
            "   [0.5921569  0.5921569  0.5921569 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.3882353  0.3882353  0.3882353 ]\n",
            "   [0.39607844 0.39607844 0.39607844]\n",
            "   [0.39607844 0.39607844 0.39607844]\n",
            "   ...\n",
            "   [0.45882353 0.45882353 0.45882353]\n",
            "   [0.4627451  0.4627451  0.4627451 ]\n",
            "   [0.49411765 0.49411765 0.49411765]]\n",
            "\n",
            "  [[0.3882353  0.3882353  0.3882353 ]\n",
            "   [0.39607844 0.39607844 0.39607844]\n",
            "   [0.39607844 0.39607844 0.39607844]\n",
            "   ...\n",
            "   [0.45882353 0.45882353 0.45882353]\n",
            "   [0.4627451  0.4627451  0.4627451 ]\n",
            "   [0.4862745  0.4862745  0.4862745 ]]\n",
            "\n",
            "  [[0.39215687 0.39215687 0.39215687]\n",
            "   [0.39215687 0.39215687 0.39215687]\n",
            "   [0.39215687 0.39215687 0.39215687]\n",
            "   ...\n",
            "   [0.45882353 0.45882353 0.45882353]\n",
            "   [0.47058824 0.47058824 0.47058824]\n",
            "   [0.46666667 0.46666667 0.46666667]]]\n",
            "\n",
            "\n",
            " [[[0.44313726 0.44313726 0.44313726]\n",
            "   [0.41568628 0.41568628 0.41568628]\n",
            "   [0.39607844 0.39607844 0.39607844]\n",
            "   ...\n",
            "   [0.34117648 0.34117648 0.34117648]\n",
            "   [0.52156866 0.52156866 0.52156866]\n",
            "   [0.58431375 0.58431375 0.58431375]]\n",
            "\n",
            "  [[0.4392157  0.4392157  0.4392157 ]\n",
            "   [0.42745098 0.42745098 0.42745098]\n",
            "   [0.4117647  0.4117647  0.4117647 ]\n",
            "   ...\n",
            "   [0.34117648 0.34117648 0.34117648]\n",
            "   [0.48235294 0.48235294 0.48235294]\n",
            "   [0.5921569  0.5921569  0.5921569 ]]\n",
            "\n",
            "  [[0.4392157  0.4392157  0.4392157 ]\n",
            "   [0.40784314 0.40784314 0.40784314]\n",
            "   [0.38431373 0.38431373 0.38431373]\n",
            "   ...\n",
            "   [0.3372549  0.3372549  0.3372549 ]\n",
            "   [0.4509804  0.4509804  0.4509804 ]\n",
            "   [0.59607846 0.59607846 0.59607846]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.13333334 0.13333334 0.13333334]\n",
            "   [0.13725491 0.13725491 0.13725491]\n",
            "   [0.10588235 0.10588235 0.10588235]\n",
            "   ...\n",
            "   [0.07058824 0.07058824 0.07058824]\n",
            "   [0.07843138 0.07843138 0.07843138]\n",
            "   [0.10196079 0.10196079 0.10196079]]\n",
            "\n",
            "  [[0.13725491 0.13725491 0.13725491]\n",
            "   [0.11764706 0.11764706 0.11764706]\n",
            "   [0.09411765 0.09411765 0.09411765]\n",
            "   ...\n",
            "   [0.09803922 0.09803922 0.09803922]\n",
            "   [0.13333334 0.13333334 0.13333334]\n",
            "   [0.16862746 0.16862746 0.16862746]]\n",
            "\n",
            "  [[0.09019608 0.09019608 0.09019608]\n",
            "   [0.09019608 0.09019608 0.09019608]\n",
            "   [0.07058824 0.07058824 0.07058824]\n",
            "   ...\n",
            "   [0.1882353  0.1882353  0.1882353 ]\n",
            "   [0.20392157 0.20392157 0.20392157]\n",
            "   [0.23529412 0.23529412 0.23529412]]]\n",
            "\n",
            "\n",
            " [[[0.2        0.2        0.2       ]\n",
            "   [0.20392157 0.20392157 0.20392157]\n",
            "   [0.20784314 0.20784314 0.20784314]\n",
            "   ...\n",
            "   [0.28627452 0.28627452 0.28627452]\n",
            "   [0.2784314  0.2784314  0.2784314 ]\n",
            "   [0.27058825 0.27058825 0.27058825]]\n",
            "\n",
            "  [[0.19215687 0.19215687 0.19215687]\n",
            "   [0.19215687 0.19215687 0.19215687]\n",
            "   [0.20392157 0.20392157 0.20392157]\n",
            "   ...\n",
            "   [0.28627452 0.28627452 0.28627452]\n",
            "   [0.26666668 0.26666668 0.26666668]\n",
            "   [0.25882354 0.25882354 0.25882354]]\n",
            "\n",
            "  [[0.19215687 0.19215687 0.19215687]\n",
            "   [0.2        0.2        0.2       ]\n",
            "   [0.20392157 0.20392157 0.20392157]\n",
            "   ...\n",
            "   [0.2901961  0.2901961  0.2901961 ]\n",
            "   [0.27058825 0.27058825 0.27058825]\n",
            "   [0.26666668 0.26666668 0.26666668]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.44705883 0.44705883 0.44705883]\n",
            "   [0.9137255  0.9137255  0.9137255 ]\n",
            "   [0.96862745 0.96862745 0.96862745]\n",
            "   ...\n",
            "   [0.7411765  0.7411765  0.7411765 ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[0.90588236 0.90588236 0.90588236]\n",
            "   [1.         1.         1.        ]\n",
            "   [0.9254902  0.9254902  0.9254902 ]\n",
            "   ...\n",
            "   [0.9372549  0.9372549  0.9372549 ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]\n",
            "\n",
            "  [[0.99215686 0.99215686 0.99215686]\n",
            "   [0.99607843 0.99607843 0.99607843]\n",
            "   [0.9647059  0.9647059  0.9647059 ]\n",
            "   ...\n",
            "   [0.9843137  0.9843137  0.9843137 ]\n",
            "   [1.         1.         1.        ]\n",
            "   [1.         1.         1.        ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[0.         0.         0.        ]\n",
            "   [0.         0.         0.        ]\n",
            "   [0.         0.         0.        ]\n",
            "   ...\n",
            "   [0.00784314 0.00784314 0.00784314]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   [0.00784314 0.00784314 0.00784314]]\n",
            "\n",
            "  [[0.         0.         0.        ]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   ...\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   [0.         0.         0.        ]\n",
            "   [0.00392157 0.00392157 0.00392157]]\n",
            "\n",
            "  [[0.00392157 0.00392157 0.00392157]\n",
            "   [0.01176471 0.01176471 0.01176471]\n",
            "   [0.01176471 0.01176471 0.01176471]\n",
            "   ...\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   [0.00392157 0.00392157 0.00392157]\n",
            "   [0.00392157 0.00392157 0.00392157]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.1764706  0.1764706  0.1764706 ]\n",
            "   [0.05098039 0.05098039 0.05098039]\n",
            "   [0.01176471 0.01176471 0.01176471]\n",
            "   ...\n",
            "   [0.05098039 0.05098039 0.05098039]\n",
            "   [0.05490196 0.05490196 0.05490196]\n",
            "   [0.06666667 0.06666667 0.06666667]]\n",
            "\n",
            "  [[0.10980392 0.10980392 0.10980392]\n",
            "   [0.02745098 0.02745098 0.02745098]\n",
            "   [0.01176471 0.01176471 0.01176471]\n",
            "   ...\n",
            "   [0.05490196 0.05490196 0.05490196]\n",
            "   [0.05882353 0.05882353 0.05882353]\n",
            "   [0.07450981 0.07450981 0.07450981]]\n",
            "\n",
            "  [[0.05490196 0.05490196 0.05490196]\n",
            "   [0.01960784 0.01960784 0.01960784]\n",
            "   [0.00784314 0.00784314 0.00784314]\n",
            "   ...\n",
            "   [0.07058824 0.07058824 0.07058824]\n",
            "   [0.07843138 0.07843138 0.07843138]\n",
            "   [0.10588235 0.10588235 0.10588235]]]\n",
            "\n",
            "\n",
            " [[[0.33333334 0.33333334 0.33333334]\n",
            "   [0.08627451 0.08627451 0.08627451]\n",
            "   [0.05098039 0.05098039 0.05098039]\n",
            "   ...\n",
            "   [0.22745098 0.22745098 0.22745098]\n",
            "   [0.8117647  0.8117647  0.8117647 ]\n",
            "   [0.9647059  0.9647059  0.9647059 ]]\n",
            "\n",
            "  [[0.16078432 0.16078432 0.16078432]\n",
            "   [0.05882353 0.05882353 0.05882353]\n",
            "   [0.03921569 0.03921569 0.03921569]\n",
            "   ...\n",
            "   [0.08627451 0.08627451 0.08627451]\n",
            "   [0.5294118  0.5294118  0.5294118 ]\n",
            "   [0.9529412  0.9529412  0.9529412 ]]\n",
            "\n",
            "  [[0.09411765 0.09411765 0.09411765]\n",
            "   [0.04313726 0.04313726 0.04313726]\n",
            "   [0.05882353 0.05882353 0.05882353]\n",
            "   ...\n",
            "   [0.02745098 0.02745098 0.02745098]\n",
            "   [0.3019608  0.3019608  0.3019608 ]\n",
            "   [0.8862745  0.8862745  0.8862745 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.8666667  0.8666667  0.8666667 ]\n",
            "   [0.8784314  0.8784314  0.8784314 ]\n",
            "   [0.8862745  0.8862745  0.8862745 ]\n",
            "   ...\n",
            "   [0.62352943 0.62352943 0.62352943]\n",
            "   [0.6313726  0.6313726  0.6313726 ]\n",
            "   [0.6392157  0.6392157  0.6392157 ]]\n",
            "\n",
            "  [[0.8862745  0.8862745  0.8862745 ]\n",
            "   [0.8901961  0.8901961  0.8901961 ]\n",
            "   [0.89411765 0.89411765 0.89411765]\n",
            "   ...\n",
            "   [0.6156863  0.6156863  0.6156863 ]\n",
            "   [0.61960787 0.61960787 0.61960787]\n",
            "   [0.6313726  0.6313726  0.6313726 ]]\n",
            "\n",
            "  [[0.90588236 0.90588236 0.90588236]\n",
            "   [0.91764706 0.91764706 0.91764706]\n",
            "   [0.91764706 0.91764706 0.91764706]\n",
            "   ...\n",
            "   [0.6039216  0.6039216  0.6039216 ]\n",
            "   [0.6117647  0.6117647  0.6117647 ]\n",
            "   [0.62352943 0.62352943 0.62352943]]]\n",
            "\n",
            "\n",
            " [[[0.07843138 0.07843138 0.07843138]\n",
            "   [0.11372549 0.11372549 0.11372549]\n",
            "   [0.2        0.2        0.2       ]\n",
            "   ...\n",
            "   [0.41568628 0.41568628 0.41568628]\n",
            "   [0.44705883 0.44705883 0.44705883]\n",
            "   [0.45490196 0.45490196 0.45490196]]\n",
            "\n",
            "  [[0.09411765 0.09411765 0.09411765]\n",
            "   [0.14509805 0.14509805 0.14509805]\n",
            "   [0.22745098 0.22745098 0.22745098]\n",
            "   ...\n",
            "   [0.25882354 0.25882354 0.25882354]\n",
            "   [0.34509805 0.34509805 0.34509805]\n",
            "   [0.45490196 0.45490196 0.45490196]]\n",
            "\n",
            "  [[0.10196079 0.10196079 0.10196079]\n",
            "   [0.15686275 0.15686275 0.15686275]\n",
            "   [0.20784314 0.20784314 0.20784314]\n",
            "   ...\n",
            "   [0.11372549 0.11372549 0.11372549]\n",
            "   [0.17254902 0.17254902 0.17254902]\n",
            "   [0.27450982 0.27450982 0.27450982]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0.34901962 0.34901962 0.34901962]\n",
            "   [0.4745098  0.4745098  0.4745098 ]\n",
            "   [0.52156866 0.52156866 0.52156866]\n",
            "   ...\n",
            "   [0.5568628  0.5568628  0.5568628 ]\n",
            "   [0.4        0.4        0.4       ]\n",
            "   [0.3137255  0.3137255  0.3137255 ]]\n",
            "\n",
            "  [[0.50980395 0.50980395 0.50980395]\n",
            "   [0.5294118  0.5294118  0.5294118 ]\n",
            "   [0.5294118  0.5294118  0.5294118 ]\n",
            "   ...\n",
            "   [0.59607846 0.59607846 0.59607846]\n",
            "   [0.57254905 0.57254905 0.57254905]\n",
            "   [0.37254903 0.37254903 0.37254903]]\n",
            "\n",
            "  [[0.54901963 0.54901963 0.54901963]\n",
            "   [0.53333336 0.53333336 0.53333336]\n",
            "   [0.5176471  0.5176471  0.5176471 ]\n",
            "   ...\n",
            "   [0.60784316 0.60784316 0.60784316]\n",
            "   [0.6117647  0.6117647  0.6117647 ]\n",
            "   [0.5647059  0.5647059  0.5647059 ]]]], shape=(12, 48, 48, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# making model\n",
        "model = tf.keras.Sequential([\n",
        "    \n",
        "\n",
        "    # .conv2D : with feature extraction we can expect higher accuracy               \n",
        "    # 1st param: make 32 different features. use such features to train better model. \n",
        "    # 2nd param: kernel size is 3x3\n",
        "    # 3rd param: when using convolutional layer, (=when kernel is applied to each photo) the size of photo will shrink. we will put some padding here to retain 48*48 size even after feature extraction.\n",
        "    # 4th param: activation function. relu compress number to 0~1. There's no negative number in photo so it is good to use relu.\n",
        "    # 5th param: input_shape : shate of one photo data\n",
        "    tf.keras.layers.Conv2D( 32, (3,3), padding='same' , activation='relu', input_shape=(48,48,3)), \n",
        "    \n",
        "    \n",
        "    # pooling layer (Downsampling)\n",
        "    # downsize 2,2 pixels to 1,1\n",
        "    tf.keras.layers.MaxPooling2D((2,2)),\n",
        "\n",
        "    # helps prevent overfittng ( randomly sets input units to 0 with frequency of rate)\n",
        "    tf.keras.layers.Dropout(0.2), \n",
        "\n",
        "    # It is okay to repreat [Conv-Pooling] several times\n",
        "    # we will repeat [Conv-Pooling] two more times to increase learning effect\n",
        "    tf.keras.layers.Conv2D( 64, (3,3), padding='same' , activation='relu'), \n",
        "    tf.keras.layers.MaxPooling2D((2,2)), \n",
        "        \n",
        "    tf.keras.layers.Conv2D( 128, (3,3), padding='same' , activation='relu'), \n",
        "    tf.keras.layers.MaxPooling2D((2,2)), \n",
        "\n",
        "\n",
        "    # Flatten-Dense\n",
        "    tf.keras.layers.Flatten(), \n",
        "    ## 1st layer\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\"), # number inside Dense is the number of nodes. usually use 2^n\n",
        "    tf.keras.layers.Dropout(0.2), # helps prevent overfittng\n",
        "    ## second layer. we must have 7 final nodes because we have 7 classes.\n",
        "    tf.keras.layers.Dense(7, activation=\"softmax\") # softmax: compress resulting number between 0~1, used in category problems. If you add up the probability of each class we get 1. \n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# prints summary of our model. \n",
        "model.summary()\n",
        "\n",
        "\n",
        "# compile and fit our model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\" , metrics=['accuracy'])\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi4bprpl1_3u",
        "outputId": "22c60923-5deb-4d96-9177-08e5b8a0f982"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 48, 48, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 24, 24, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 24, 24, 32)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 12, 12, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 12, 12, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4608)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                294976    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7)                 455       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 388,679\n",
            "Trainable params: 388,679\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "58/58 [==============================] - 10s 147ms/step - loss: 1.8762 - accuracy: 0.2136 - val_loss: 1.8370 - val_accuracy: 0.2500\n",
            "Epoch 2/10\n",
            "58/58 [==============================] - 6s 94ms/step - loss: 1.8054 - accuracy: 0.2900 - val_loss: 1.6531 - val_accuracy: 0.3819\n",
            "Epoch 3/10\n",
            "58/58 [==============================] - 5s 92ms/step - loss: 1.3467 - accuracy: 0.4993 - val_loss: 1.0217 - val_accuracy: 0.5625\n",
            "Epoch 4/10\n",
            "58/58 [==============================] - 6s 93ms/step - loss: 0.9102 - accuracy: 0.6465 - val_loss: 0.8974 - val_accuracy: 0.6007\n",
            "Epoch 5/10\n",
            "58/58 [==============================] - 6s 93ms/step - loss: 0.7178 - accuracy: 0.6883 - val_loss: 0.7879 - val_accuracy: 0.6910\n",
            "Epoch 6/10\n",
            "58/58 [==============================] - 6s 95ms/step - loss: 0.5514 - accuracy: 0.7648 - val_loss: 0.6898 - val_accuracy: 0.7674\n",
            "Epoch 7/10\n",
            "58/58 [==============================] - 7s 127ms/step - loss: 0.4606 - accuracy: 0.8023 - val_loss: 0.7358 - val_accuracy: 0.6875\n",
            "Epoch 8/10\n",
            "58/58 [==============================] - 5s 92ms/step - loss: 0.4143 - accuracy: 0.8499 - val_loss: 0.6193 - val_accuracy: 0.7778\n",
            "Epoch 9/10\n",
            "58/58 [==============================] - 6s 93ms/step - loss: 0.2886 - accuracy: 0.8889 - val_loss: 0.6390 - val_accuracy: 0.7708\n",
            "Epoch 10/10\n",
            "58/58 [==============================] - 6s 95ms/step - loss: 0.3345 - accuracy: 0.8831 - val_loss: 0.7755 - val_accuracy: 0.7569\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcf663c58d0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}