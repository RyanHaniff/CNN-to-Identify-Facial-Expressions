{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MyQ4QFbZVhuG"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UB6z6XFaWdcv",
    "outputId": "c1b8540b-5e75-4fdd-c50e-6f4d89436a0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1440 files belonging to 7 classes.\n",
      "Found 2760 files belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# get training and validation(=testing) data\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '../COMP_473_Project/TestSet1-new',\n",
    "    image_size = (48,48), # our original dataset is 48 pixels by 48 pixels\n",
    "    batch_size = 12 # pick 12 images and trin until all dataset is used <= repeat this for each epoche\n",
    ")\n",
    "\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '../COMP_473_Project/TrainSet1-new',\n",
    "    image_size = (48,48), \n",
    "    batch_size = 12\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gDTPOOojvWc4",
    "outputId": "307a2019-d9ec-4c0c-9bfa-7c02d28c28b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<BatchDataset element_spec=(TensorSpec(shape=(None, 48, 48, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds \n",
    "# form shape=(None, 48, 48, 3) <- 3 represent that the photo is treated as colored. one pixel will have [R G B] values between 0~255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "nlxVewnivXWB",
    "outputId": "cdfa1795-48ef-413a-c428-962b243be067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[214. 214. 214.]\n",
      "   [214. 214. 214.]\n",
      "   [214. 214. 214.]\n",
      "   ...\n",
      "   [212. 212. 212.]\n",
      "   [211. 211. 211.]\n",
      "   [211. 211. 211.]]\n",
      "\n",
      "  [[214. 214. 214.]\n",
      "   [214. 214. 214.]\n",
      "   [214. 214. 214.]\n",
      "   ...\n",
      "   [211. 211. 211.]\n",
      "   [211. 211. 211.]\n",
      "   [211. 211. 211.]]\n",
      "\n",
      "  [[214. 214. 214.]\n",
      "   [214. 214. 214.]\n",
      "   [214. 214. 214.]\n",
      "   ...\n",
      "   [210. 210. 210.]\n",
      "   [211. 211. 211.]\n",
      "   [211. 211. 211.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[209. 209. 209.]\n",
      "   [208. 208. 208.]\n",
      "   [207. 207. 207.]\n",
      "   ...\n",
      "   [209. 209. 209.]\n",
      "   [209. 209. 209.]\n",
      "   [208. 208. 208.]]\n",
      "\n",
      "  [[207. 207. 207.]\n",
      "   [208. 208. 208.]\n",
      "   [206. 206. 206.]\n",
      "   ...\n",
      "   [209. 209. 209.]\n",
      "   [209. 209. 209.]\n",
      "   [208. 208. 208.]]\n",
      "\n",
      "  [[207. 207. 207.]\n",
      "   [207. 207. 207.]\n",
      "   [206. 206. 206.]\n",
      "   ...\n",
      "   [210. 210. 210.]\n",
      "   [209. 209. 209.]\n",
      "   [208. 208. 208.]]]\n",
      "\n",
      "\n",
      " [[[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [211. 211. 211.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [ 20.  20.  20.]\n",
      "   [ 34.  34.  34.]\n",
      "   ...\n",
      "   [210. 210. 210.]\n",
      "   [ 20.  20.  20.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[214. 214. 214.]\n",
      "   [214. 214. 214.]\n",
      "   [215. 215. 215.]\n",
      "   ...\n",
      "   [210. 210. 210.]\n",
      "   [ 33.  33.  33.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [ 53.  53.  53.]\n",
      "   [210. 210. 210.]\n",
      "   ...\n",
      "   [210. 210. 210.]\n",
      "   [210. 210. 210.]\n",
      "   [209. 209. 209.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [ 33.  33.  33.]\n",
      "   [209. 209. 209.]\n",
      "   ...\n",
      "   [ 53.  53.  53.]\n",
      "   [ 33.  33.  33.]\n",
      "   [ 20.  20.  20.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [ 20.  20.  20.]\n",
      "   [208. 208. 208.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]]\n",
      "\n",
      "\n",
      " [[[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [ 16.  16.  16.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  1.   1.   1.]\n",
      "   [ 14.  14.  14.]\n",
      "   ...\n",
      "   [ 16.  16.  16.]\n",
      "   [ 13.  13.  13.]\n",
      "   [  9.   9.   9.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  3.   3.   3.]\n",
      "   [ 21.  21.  21.]\n",
      "   ...\n",
      "   [ 64.  64.  64.]\n",
      "   [ 80.  80.  80.]\n",
      "   [ 93.  93.  93.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 61.  61.  61.]\n",
      "   [ 63.  63.  63.]\n",
      "   [ 63.  63.  63.]\n",
      "   ...\n",
      "   [ 76.  76.  76.]\n",
      "   [ 19.  19.  19.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  6.   6.   6.]\n",
      "   [ 10.  10.  10.]\n",
      "   ...\n",
      "   [ 77.  77.  77.]\n",
      "   [ 12.  12.  12.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [ 76.  76.  76.]\n",
      "   [  7.   7.   7.]\n",
      "   [  0.   0.   0.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[155. 155. 155.]\n",
      "   [149. 149. 149.]\n",
      "   [131. 131. 131.]\n",
      "   ...\n",
      "   [ 72.  72.  72.]\n",
      "   [ 89.  89.  89.]\n",
      "   [104. 104. 104.]]\n",
      "\n",
      "  [[153. 153. 153.]\n",
      "   [142. 142. 142.]\n",
      "   [124. 124. 124.]\n",
      "   ...\n",
      "   [ 72.  72.  72.]\n",
      "   [ 67.  67.  67.]\n",
      "   [ 90.  90.  90.]]\n",
      "\n",
      "  [[149. 149. 149.]\n",
      "   [128. 128. 128.]\n",
      "   [129. 129. 129.]\n",
      "   ...\n",
      "   [ 79.  79.  79.]\n",
      "   [ 65.  65.  65.]\n",
      "   [ 72.  72.  72.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[110. 110. 110.]\n",
      "   [107. 107. 107.]\n",
      "   [ 95.  95.  95.]\n",
      "   ...\n",
      "   [ 89.  89.  89.]\n",
      "   [ 67.  67.  67.]\n",
      "   [108. 108. 108.]]\n",
      "\n",
      "  [[113. 113. 113.]\n",
      "   [108. 108. 108.]\n",
      "   [ 96.  96.  96.]\n",
      "   ...\n",
      "   [ 99.  99.  99.]\n",
      "   [ 72.  72.  72.]\n",
      "   [ 83.  83.  83.]]\n",
      "\n",
      "  [[117. 117. 117.]\n",
      "   [111. 111. 111.]\n",
      "   [ 94.  94.  94.]\n",
      "   ...\n",
      "   [105. 105. 105.]\n",
      "   [ 80.  80.  80.]\n",
      "   [ 79.  79.  79.]]]\n",
      "\n",
      "\n",
      " [[[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]]\n",
      "\n",
      "\n",
      " [[[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [ 79.  79.  79.]\n",
      "   ...\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  7.   7.   7.]\n",
      "   [ 78.  78.  78.]\n",
      "   ...\n",
      "   [ 18.  18.  18.]\n",
      "   [ 12.  12.  12.]\n",
      "   [  8.   8.   8.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [ 12.  12.  12.]\n",
      "   [ 74.  74.  74.]\n",
      "   ...\n",
      "   [ 73.  73.  73.]\n",
      "   [ 78.  78.  78.]\n",
      "   [ 82.  82.  82.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 71.  71.  71.]\n",
      "   [ 71.  71.  71.]\n",
      "   [ 72.  72.  72.]\n",
      "   ...\n",
      "   [ 81.  81.  81.]\n",
      "   [ 20.  20.  20.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  7.   7.   7.]\n",
      "   [ 11.  11.  11.]\n",
      "   ...\n",
      "   [ 82.  82.  82.]\n",
      "   [ 13.  13.  13.]\n",
      "   [  0.   0.   0.]]\n",
      "\n",
      "  [[  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   [  0.   0.   0.]\n",
      "   ...\n",
      "   [ 81.  81.  81.]\n",
      "   [  8.   8.   8.]\n",
      "   [  0.   0.   0.]]]], shape=(12, 48, 48, 3), dtype=float32)\n",
      "tf.Tensor([1 1 2 4 6 6 3 4 0 1 5 6], shape=(12,), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try to output one of our image\n",
    "\n",
    "for i, answer in train_ds.take(1): #take(1): takes first batch. \n",
    "\n",
    "  print(i) # outputs 12(=batch size) photos as numpy array ,shape=(12, 48, 48, 3) means 12 photos, each photo size(48,48 pixels), 3=color= each cell represented as [R G B]\n",
    "  print(answer) # outputs correct answer for current batch [ 5 2 6 ...] <- first photo in this batch is in class 5, second photo in this patch is in class 2.. \n",
    "\n",
    "plt.imshow(i[0].numpy().astype('uint8')) # output the first photo using matplotlib\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pEoV-cdH1lGq"
   },
   "outputs": [],
   "source": [
    "# preprocess colored data\n",
    "# since the numbers are between 0~255 for each R, G, and B, it is slower to train and calculate weights.\n",
    "# it is better to divide each number with 255, so each value resides within 0~1\n",
    "\n",
    "def processColoredData(i, answer):\n",
    "  i=tf.cast(i/255.0, tf.float32) # devide i by 255, resulting data type should be float\n",
    "  return i, answer\n",
    "\n",
    "train_ds = train_ds.map(processColoredData)\n",
    "val_ds = val_ds.map(processColoredData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bid7gMcW1se5",
    "outputId": "b760f03f-92c5-49d5-cf67-54a4d31e2849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0.4745098  0.4745098  0.4745098 ]\n",
      "   [0.4117647  0.4117647  0.4117647 ]\n",
      "   [0.34509805 0.34509805 0.34509805]\n",
      "   ...\n",
      "   [0.5764706  0.5764706  0.5764706 ]\n",
      "   [0.56078434 0.56078434 0.56078434]\n",
      "   [0.57254905 0.57254905 0.57254905]]\n",
      "\n",
      "  [[0.41568628 0.41568628 0.41568628]\n",
      "   [0.37254903 0.37254903 0.37254903]\n",
      "   [0.34901962 0.34901962 0.34901962]\n",
      "   ...\n",
      "   [0.60784316 0.60784316 0.60784316]\n",
      "   [0.6156863  0.6156863  0.6156863 ]\n",
      "   [0.5921569  0.5921569  0.5921569 ]]\n",
      "\n",
      "  [[0.37254903 0.37254903 0.37254903]\n",
      "   [0.29411766 0.29411766 0.29411766]\n",
      "   [0.3254902  0.3254902  0.3254902 ]\n",
      "   ...\n",
      "   [0.654902   0.654902   0.654902  ]\n",
      "   [0.6392157  0.6392157  0.6392157 ]\n",
      "   [0.5921569  0.5921569  0.5921569 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.3882353  0.3882353  0.3882353 ]\n",
      "   [0.39607844 0.39607844 0.39607844]\n",
      "   [0.39607844 0.39607844 0.39607844]\n",
      "   ...\n",
      "   [0.45882353 0.45882353 0.45882353]\n",
      "   [0.4627451  0.4627451  0.4627451 ]\n",
      "   [0.49411765 0.49411765 0.49411765]]\n",
      "\n",
      "  [[0.3882353  0.3882353  0.3882353 ]\n",
      "   [0.39607844 0.39607844 0.39607844]\n",
      "   [0.39607844 0.39607844 0.39607844]\n",
      "   ...\n",
      "   [0.45882353 0.45882353 0.45882353]\n",
      "   [0.4627451  0.4627451  0.4627451 ]\n",
      "   [0.4862745  0.4862745  0.4862745 ]]\n",
      "\n",
      "  [[0.39215687 0.39215687 0.39215687]\n",
      "   [0.39215687 0.39215687 0.39215687]\n",
      "   [0.39215687 0.39215687 0.39215687]\n",
      "   ...\n",
      "   [0.45882353 0.45882353 0.45882353]\n",
      "   [0.47058824 0.47058824 0.47058824]\n",
      "   [0.46666667 0.46666667 0.46666667]]]\n",
      "\n",
      "\n",
      " [[[0.44313726 0.44313726 0.44313726]\n",
      "   [0.41568628 0.41568628 0.41568628]\n",
      "   [0.39607844 0.39607844 0.39607844]\n",
      "   ...\n",
      "   [0.34117648 0.34117648 0.34117648]\n",
      "   [0.52156866 0.52156866 0.52156866]\n",
      "   [0.58431375 0.58431375 0.58431375]]\n",
      "\n",
      "  [[0.4392157  0.4392157  0.4392157 ]\n",
      "   [0.42745098 0.42745098 0.42745098]\n",
      "   [0.4117647  0.4117647  0.4117647 ]\n",
      "   ...\n",
      "   [0.34117648 0.34117648 0.34117648]\n",
      "   [0.48235294 0.48235294 0.48235294]\n",
      "   [0.5921569  0.5921569  0.5921569 ]]\n",
      "\n",
      "  [[0.4392157  0.4392157  0.4392157 ]\n",
      "   [0.40784314 0.40784314 0.40784314]\n",
      "   [0.38431373 0.38431373 0.38431373]\n",
      "   ...\n",
      "   [0.3372549  0.3372549  0.3372549 ]\n",
      "   [0.4509804  0.4509804  0.4509804 ]\n",
      "   [0.59607846 0.59607846 0.59607846]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.13333334 0.13333334 0.13333334]\n",
      "   [0.13725491 0.13725491 0.13725491]\n",
      "   [0.10588235 0.10588235 0.10588235]\n",
      "   ...\n",
      "   [0.07058824 0.07058824 0.07058824]\n",
      "   [0.07843138 0.07843138 0.07843138]\n",
      "   [0.10196079 0.10196079 0.10196079]]\n",
      "\n",
      "  [[0.13725491 0.13725491 0.13725491]\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   [0.09411765 0.09411765 0.09411765]\n",
      "   ...\n",
      "   [0.09803922 0.09803922 0.09803922]\n",
      "   [0.13333334 0.13333334 0.13333334]\n",
      "   [0.16862746 0.16862746 0.16862746]]\n",
      "\n",
      "  [[0.09019608 0.09019608 0.09019608]\n",
      "   [0.09019608 0.09019608 0.09019608]\n",
      "   [0.07058824 0.07058824 0.07058824]\n",
      "   ...\n",
      "   [0.1882353  0.1882353  0.1882353 ]\n",
      "   [0.20392157 0.20392157 0.20392157]\n",
      "   [0.23529412 0.23529412 0.23529412]]]\n",
      "\n",
      "\n",
      " [[[0.2        0.2        0.2       ]\n",
      "   [0.20392157 0.20392157 0.20392157]\n",
      "   [0.20784314 0.20784314 0.20784314]\n",
      "   ...\n",
      "   [0.28627452 0.28627452 0.28627452]\n",
      "   [0.2784314  0.2784314  0.2784314 ]\n",
      "   [0.27058825 0.27058825 0.27058825]]\n",
      "\n",
      "  [[0.19215687 0.19215687 0.19215687]\n",
      "   [0.19215687 0.19215687 0.19215687]\n",
      "   [0.20392157 0.20392157 0.20392157]\n",
      "   ...\n",
      "   [0.28627452 0.28627452 0.28627452]\n",
      "   [0.26666668 0.26666668 0.26666668]\n",
      "   [0.25882354 0.25882354 0.25882354]]\n",
      "\n",
      "  [[0.19215687 0.19215687 0.19215687]\n",
      "   [0.2        0.2        0.2       ]\n",
      "   [0.20392157 0.20392157 0.20392157]\n",
      "   ...\n",
      "   [0.2901961  0.2901961  0.2901961 ]\n",
      "   [0.27058825 0.27058825 0.27058825]\n",
      "   [0.26666668 0.26666668 0.26666668]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.44705883 0.44705883 0.44705883]\n",
      "   [0.9137255  0.9137255  0.9137255 ]\n",
      "   [0.96862745 0.96862745 0.96862745]\n",
      "   ...\n",
      "   [0.7411765  0.7411765  0.7411765 ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[0.90588236 0.90588236 0.90588236]\n",
      "   [1.         1.         1.        ]\n",
      "   [0.9254902  0.9254902  0.9254902 ]\n",
      "   ...\n",
      "   [0.9372549  0.9372549  0.9372549 ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]\n",
      "\n",
      "  [[0.99215686 0.99215686 0.99215686]\n",
      "   [0.99607843 0.99607843 0.99607843]\n",
      "   [0.9647059  0.9647059  0.9647059 ]\n",
      "   ...\n",
      "   [0.9843137  0.9843137  0.9843137 ]\n",
      "   [1.         1.         1.        ]\n",
      "   [1.         1.         1.        ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.00784314 0.00784314 0.00784314]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00784314 0.00784314 0.00784314]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.00392157 0.00392157 0.00392157]]\n",
      "\n",
      "  [[0.00392157 0.00392157 0.00392157]\n",
      "   [0.01176471 0.01176471 0.01176471]\n",
      "   [0.01176471 0.01176471 0.01176471]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.1764706  0.1764706  0.1764706 ]\n",
      "   [0.05098039 0.05098039 0.05098039]\n",
      "   [0.01176471 0.01176471 0.01176471]\n",
      "   ...\n",
      "   [0.05098039 0.05098039 0.05098039]\n",
      "   [0.05490196 0.05490196 0.05490196]\n",
      "   [0.06666667 0.06666667 0.06666667]]\n",
      "\n",
      "  [[0.10980392 0.10980392 0.10980392]\n",
      "   [0.02745098 0.02745098 0.02745098]\n",
      "   [0.01176471 0.01176471 0.01176471]\n",
      "   ...\n",
      "   [0.05490196 0.05490196 0.05490196]\n",
      "   [0.05882353 0.05882353 0.05882353]\n",
      "   [0.07450981 0.07450981 0.07450981]]\n",
      "\n",
      "  [[0.05490196 0.05490196 0.05490196]\n",
      "   [0.01960784 0.01960784 0.01960784]\n",
      "   [0.00784314 0.00784314 0.00784314]\n",
      "   ...\n",
      "   [0.07058824 0.07058824 0.07058824]\n",
      "   [0.07843138 0.07843138 0.07843138]\n",
      "   [0.10588235 0.10588235 0.10588235]]]\n",
      "\n",
      "\n",
      " [[[0.33333334 0.33333334 0.33333334]\n",
      "   [0.08627451 0.08627451 0.08627451]\n",
      "   [0.05098039 0.05098039 0.05098039]\n",
      "   ...\n",
      "   [0.22745098 0.22745098 0.22745098]\n",
      "   [0.8117647  0.8117647  0.8117647 ]\n",
      "   [0.9647059  0.9647059  0.9647059 ]]\n",
      "\n",
      "  [[0.16078432 0.16078432 0.16078432]\n",
      "   [0.05882353 0.05882353 0.05882353]\n",
      "   [0.03921569 0.03921569 0.03921569]\n",
      "   ...\n",
      "   [0.08627451 0.08627451 0.08627451]\n",
      "   [0.5294118  0.5294118  0.5294118 ]\n",
      "   [0.9529412  0.9529412  0.9529412 ]]\n",
      "\n",
      "  [[0.09411765 0.09411765 0.09411765]\n",
      "   [0.04313726 0.04313726 0.04313726]\n",
      "   [0.05882353 0.05882353 0.05882353]\n",
      "   ...\n",
      "   [0.02745098 0.02745098 0.02745098]\n",
      "   [0.3019608  0.3019608  0.3019608 ]\n",
      "   [0.8862745  0.8862745  0.8862745 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.8666667  0.8666667  0.8666667 ]\n",
      "   [0.8784314  0.8784314  0.8784314 ]\n",
      "   [0.8862745  0.8862745  0.8862745 ]\n",
      "   ...\n",
      "   [0.62352943 0.62352943 0.62352943]\n",
      "   [0.6313726  0.6313726  0.6313726 ]\n",
      "   [0.6392157  0.6392157  0.6392157 ]]\n",
      "\n",
      "  [[0.8862745  0.8862745  0.8862745 ]\n",
      "   [0.8901961  0.8901961  0.8901961 ]\n",
      "   [0.89411765 0.89411765 0.89411765]\n",
      "   ...\n",
      "   [0.6156863  0.6156863  0.6156863 ]\n",
      "   [0.61960787 0.61960787 0.61960787]\n",
      "   [0.6313726  0.6313726  0.6313726 ]]\n",
      "\n",
      "  [[0.90588236 0.90588236 0.90588236]\n",
      "   [0.91764706 0.91764706 0.91764706]\n",
      "   [0.91764706 0.91764706 0.91764706]\n",
      "   ...\n",
      "   [0.6039216  0.6039216  0.6039216 ]\n",
      "   [0.6117647  0.6117647  0.6117647 ]\n",
      "   [0.62352943 0.62352943 0.62352943]]]\n",
      "\n",
      "\n",
      " [[[0.07843138 0.07843138 0.07843138]\n",
      "   [0.11372549 0.11372549 0.11372549]\n",
      "   [0.2        0.2        0.2       ]\n",
      "   ...\n",
      "   [0.41568628 0.41568628 0.41568628]\n",
      "   [0.44705883 0.44705883 0.44705883]\n",
      "   [0.45490196 0.45490196 0.45490196]]\n",
      "\n",
      "  [[0.09411765 0.09411765 0.09411765]\n",
      "   [0.14509805 0.14509805 0.14509805]\n",
      "   [0.22745098 0.22745098 0.22745098]\n",
      "   ...\n",
      "   [0.25882354 0.25882354 0.25882354]\n",
      "   [0.34509805 0.34509805 0.34509805]\n",
      "   [0.45490196 0.45490196 0.45490196]]\n",
      "\n",
      "  [[0.10196079 0.10196079 0.10196079]\n",
      "   [0.15686275 0.15686275 0.15686275]\n",
      "   [0.20784314 0.20784314 0.20784314]\n",
      "   ...\n",
      "   [0.11372549 0.11372549 0.11372549]\n",
      "   [0.17254902 0.17254902 0.17254902]\n",
      "   [0.27450982 0.27450982 0.27450982]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.34901962 0.34901962 0.34901962]\n",
      "   [0.4745098  0.4745098  0.4745098 ]\n",
      "   [0.52156866 0.52156866 0.52156866]\n",
      "   ...\n",
      "   [0.5568628  0.5568628  0.5568628 ]\n",
      "   [0.4        0.4        0.4       ]\n",
      "   [0.3137255  0.3137255  0.3137255 ]]\n",
      "\n",
      "  [[0.50980395 0.50980395 0.50980395]\n",
      "   [0.5294118  0.5294118  0.5294118 ]\n",
      "   [0.5294118  0.5294118  0.5294118 ]\n",
      "   ...\n",
      "   [0.59607846 0.59607846 0.59607846]\n",
      "   [0.57254905 0.57254905 0.57254905]\n",
      "   [0.37254903 0.37254903 0.37254903]]\n",
      "\n",
      "  [[0.54901963 0.54901963 0.54901963]\n",
      "   [0.53333336 0.53333336 0.53333336]\n",
      "   [0.5176471  0.5176471  0.5176471 ]\n",
      "   ...\n",
      "   [0.60784316 0.60784316 0.60784316]\n",
      "   [0.6117647  0.6117647  0.6117647 ]\n",
      "   [0.5647059  0.5647059  0.5647059 ]]]], shape=(12, 48, 48, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# try to output our image if 'processColoredData' function is working well.\n",
    "for i, answer in train_ds.take(1): #take first batch \n",
    "  print(i) # now you can see that the values are compressed btw 0~1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pi4bprpl1_3u",
    "outputId": "22c60923-5deb-4d96-9177-08e5b8a0f982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 24, 24, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 24, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                294976    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 388,679\n",
      "Trainable params: 388,679\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 10s 147ms/step - loss: 1.8762 - accuracy: 0.2136 - val_loss: 1.8370 - val_accuracy: 0.2500\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 6s 94ms/step - loss: 1.8054 - accuracy: 0.2900 - val_loss: 1.6531 - val_accuracy: 0.3819\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 5s 92ms/step - loss: 1.3467 - accuracy: 0.4993 - val_loss: 1.0217 - val_accuracy: 0.5625\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 6s 93ms/step - loss: 0.9102 - accuracy: 0.6465 - val_loss: 0.8974 - val_accuracy: 0.6007\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 6s 93ms/step - loss: 0.7178 - accuracy: 0.6883 - val_loss: 0.7879 - val_accuracy: 0.6910\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 6s 95ms/step - loss: 0.5514 - accuracy: 0.7648 - val_loss: 0.6898 - val_accuracy: 0.7674\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 7s 127ms/step - loss: 0.4606 - accuracy: 0.8023 - val_loss: 0.7358 - val_accuracy: 0.6875\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 5s 92ms/step - loss: 0.4143 - accuracy: 0.8499 - val_loss: 0.6193 - val_accuracy: 0.7778\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 6s 93ms/step - loss: 0.2886 - accuracy: 0.8889 - val_loss: 0.6390 - val_accuracy: 0.7708\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - 6s 95ms/step - loss: 0.3345 - accuracy: 0.8831 - val_loss: 0.7755 - val_accuracy: 0.7569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcf663c58d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making model\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "\n",
    "    # .conv2D : with feature extraction we can expect higher accuracy               \n",
    "    # 1st param: make 32 different features. use such features to train better model. \n",
    "    # 2nd param: kernel size is 3x3\n",
    "    # 3rd param: when using convolutional layer, (=when kernel is applied to each photo) the size of photo will shrink. we will put some padding here to retain 48*48 size even after feature extraction.\n",
    "    # 4th param: activation function. relu compress number to 0~1. There's no negative number in photo so it is good to use relu.\n",
    "    # 5th param: input_shape : shape of one photo data\n",
    "    tf.keras.layers.Conv2D( 32, (3,3), padding='same' , activation='relu', input_shape=(48,48,3)), \n",
    "    \n",
    "    \n",
    "    # pooling layer (Downsampling)\n",
    "    # downsize 2,2 pixels to 1,1\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "\n",
    "    # helps prevent overfittng ( randomly sets input units to 0 with frequency of rate)\n",
    "    tf.keras.layers.Dropout(0.2), \n",
    "\n",
    "    # It is okay to repreat [Conv-Pooling] several times\n",
    "    # we will repeat [Conv-Pooling] two more times to increase learning effect\n",
    "    tf.keras.layers.Conv2D( 64, (3,3), padding='same' , activation='relu'), \n",
    "    tf.keras.layers.MaxPooling2D((2,2)), \n",
    "        \n",
    "    tf.keras.layers.Conv2D( 128, (3,3), padding='same' , activation='relu'), \n",
    "    tf.keras.layers.MaxPooling2D((2,2)), \n",
    "\n",
    "\n",
    "    # Flatten-Dense\n",
    "    tf.keras.layers.Flatten(), \n",
    "    ## 1st layer\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"), # number inside Dense is the number of nodes. usually use 2^n\n",
    "    tf.keras.layers.Dropout(0.2), # helps prevent overfittng\n",
    "    ## second layer. we must have 7 final nodes because we have 7 classes.\n",
    "    tf.keras.layers.Dense(7, activation=\"softmax\") # softmax: compress resulting number between 0~1, used in category problems. If you add up the probability of each class we get 1. \n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# prints summary of our model. \n",
    "model.summary()\n",
    "\n",
    "\n",
    "# compile and fit our model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\" , metrics=['accuracy'])\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "96762580dc771c728ac9a1b8aa29a3a420bc09545a8c1a32553175fbb1f6eb2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
